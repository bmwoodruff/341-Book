
\begin{enumerate}

\item Explain the dot product and inner product. Use them to define length and angles of vectors. Normalize vectors, obtain vectors of a given length in a given direction, and explain how to tell if two vectors are orthogonal. 

\item Obtain the orthogonal complement of a set of vectors (relate it to the null space of the transpose). Use the orthogonal complement to project vectors onto vector spaces. 

\item By considering the projection of a vector onto a vector subspace, explain why solving $A^TA \vec x = A^T\vec b$ gives the solution to the least squares regression problem. This is why statistics works.

\item Use the Gram-Schmidt orthogonalization process to obtain an orthonormal basis of vectors. Show how to compute components of vectors relative to an orthogonal basis (called Fourier coefficients).  

\item Illustrate by examples that when a matrix is symmetric, eigenvectors corresponding to different eigenvalues are orthogonal, which means you can find an orthonormal basis to diagonalize the matrix. 

\end{enumerate}
