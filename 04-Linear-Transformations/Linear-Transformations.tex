\chapter{Linear Transformations}

This chapter covers the following ideas. The purpose of this chapter is to extend the notion of functions between numbers to functions between vector spaces.  We'll see that vector subspaces related to these functions can help us solve many problems we have been looking at in previous chapters.

\input{04-Linear-Transformations/Linear-Transformations-objectives}

\section{Matrix Transformations}


In this section, we'll explore how matrices transform (reshape, move) the plane and space. 
In the next section we will use this geometric idea to define linear transformations. 
We'll then show that every matrix represents a linear transformation, and that every linear transformation (between finite dimensional vector spaces) can be represented as a matrix.  
The study of linear algebra is really the study of linear transformations between vector spaces. 
Since linear transformations can be represented as matrices, we began our study with matrices.

\subsection{Square Matrices}

Let's start by illustrating how a square matrix $A$ transform a vector $\vec x$ into another vector $A\vec x$. 
For what follows, we will think of the matrix $A$ acting on a vector $\vec x$ to product a transformed vector $A\vec x$.
For a 2 by 2 matrix such as $A = \begin{bmatrix}\cl{2\\0}&\cl{1\\3}\end{bmatrix}$, the columns of $A$ tell us where to map $\vec e_1 = (1,0)$ and $\vec e_2=(0,1)$.
The product $A\vec e_1 = (2,0)$ is the first column of $A$ and $A \vec e_2 = (1,3)$ is the second column. 
Every other vector $(x,y)$ in 2D is transformed in a similar fashion by considering the product 
\begin{align*}
A\begin{bmatrix}x\\y\end{bmatrix} 
&=A\left(x\begin{bmatrix}1\\0\end{bmatrix}+ 
y\begin{bmatrix}0\\1\end{bmatrix}\right) &\text{write in terms of basis}\\
A\begin{bmatrix}x\\y\end{bmatrix} 
&=A\begin{bmatrix}x\\0\end{bmatrix}+ 
A\begin{bmatrix}0\\y\end{bmatrix} &\text{distribute across addition}\\
&=xA\begin{bmatrix}1\\0\end{bmatrix}+ 
yA\begin{bmatrix}0\\1\end{bmatrix} &\text{pull scalars out front}\\
&=xA\vec e_1+yA\vec e_2.
\end{align*}
From the previous computation, once we know where $(1,0)$ and $(0,1)$ are transformed, we can transform every other vector by just using this information. Once know what the matrix $A$ does to a basis of $\mathbb{R}^2$, we know what the matrix does to every other vector in $\mathbb{R}^2$, because every other vector is a linear combination of these basis vectors.



We'll illustrate this transformation with 2D plots (see figure \ref{matrix transformation example}). We draw the vectors $(1,0),(0,1)$ and their images $(2,0),(1,3)$.  The unit square transforms into a parallelogram. The matrix transforms a triangle into a new triangle, a circle into an ellipse, and a heart into a slanted heart. One key observation is that the matrix transforms lines to lines, which is why we call it a linear transformation. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[inner sep=0mm]
\node (middle) {\begin{tabular}{c}
Matrix\\
$\begin{bmatrix}\cl{2\\0}&\cl{1\\3}\end{bmatrix}$\\
Determinant\\
6\\
Eigenvalues\\
2,3\\
Eigenvectors\\
(1,0), (1,1)
\end{tabular}};
\node (B) [left=of middle,xshift=1cm] {\includegraphics[height=2in]{04-Linear-Transformations/support/LT1b}};
\node (A) [left=of B,xshift=1cm] {\includegraphics[height=2in]{04-Linear-Transformations/support/LT1}};
\end{tikzpicture}
\end{center}

\caption{{
Two visualizations of a transformation. 
	Matrix transformations map (see left) lines to lines, triangles to triangles, boxes to parallelograms, and (see right) circles to ellipses.		
	Every other point is transformed in a similar fashion (the heart maps to a slanted heart). 
	The determinant measures the increase in area (the parallelogram has 6 times the area of the box, the ellipse has 6 times the area of the circle).	
	Eigenvectors tell you the directions in which the transformation moves points radially outwards. 
	Eigenvalues tell you how much outward stretching occurs in the eigenvector directions. 
}}
\label{matrix transformation example}
\end{figure}

The eigenvectors of this transformation tell us the directions in which things are transformed radially outwards 
(hence $A\vec x = \lambda \vec x$ means that the vector $\vec x$ is just stretched outward by the factor $\lambda$).  
The determinant tells us how much area (or volume) is stretched by, and if the transformation involved a flip. 
The determinant is the product of the eigenvalues, so an outward stretch of 2 in one direction followed by an outward stretch of 3 in another direction should result in increasing the area by a factor of (2)(3)=6 (see figure \ref{matrix transformation example}). 
If the determinant of our transformation is zero, then in some direction the transformation squashes things flat.
Once an object has been squashed flat, it is impossible to invert this transformation (which is why there is no inverse when the determinant is zero). 
Time for some more examples.



\begin{example}\label{details for four}

Figure \ref{four matrix transformations} illustrates the transformations given by  the four matrices 
$$A = 
\begin{bmatrix}
3&1\\1&3
\end{bmatrix}
,
B = 
\begin{bmatrix}
-1&-1\\-3&1
\end{bmatrix}
,
C = 
\begin{bmatrix}
1&1\\1&-1
\end{bmatrix}
,
D = 
\begin{bmatrix}
1&1\\-1&1
\end{bmatrix}
.$$ 

For $A$, the matrix has two positive eigenvalues. The transformation pushes everything outwards.  In the $(1,1)$ directions, lengths are multiplied by 4.  In the $(-1,1)$ direction, lengths are multiplied by 2.  This results in the area increasing by a factor of 8.  Notice that the eigenvector directions provide the major and minor axes for the ellipse.  This is always true when the matrix is symmetric.

For $B$, the matrix has a positive and negative eigenvalue.  The determinant is negative which is because the image was flipped over in the transformation.  Area is increased by a factor of 4.  The matrix is not symmetric, which is why the eigenvector directions are not the major and minor axes of the transformed ellipse.

We'll look at $C$ and $D$ simultaneously.  The difference between these two matrices is that the columns have been interchanged. Interchanging the columns results in a flip, which causes the determinant to switch signs. The area increase for both is 2.  The eigenvalues of $D$ are complex, which means there is no direction in which points are transformed radially outwards - every point is rotated some amount by this transformation.      

\newcommand{\myvfplotheight}{1.8in}
\begin{figure}
\begin{tikzpicture}

\node (symmetric) {\begin{tabular}{c}
Matrix\\
$\begin{bmatrix}\cl{3\\1}&\cl{1\\3}\end{bmatrix}$\\
Determinant\\
8\\
Eigenvalues\\
$4,2$\\
Eigenvectors\\
$(1,1), (-1,1)$
\end{tabular}};
\node [right=of symmetric] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTsymmetricb}};
\node [left=of symmetric] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTsymmetrica}};


\node (negative) [below=of symmetric] {\begin{tabular}{c}
Matrix\\
$\begin{bmatrix}\cl{-1\\-3}&\cl{-1\\1}\end{bmatrix}$\\
Determinant\\
-4\\
Eigenvalues\\
$-2,2$\\
Eigenvectors\\
$(1,1), (-1,3)$
\end{tabular}};
\node [right=of negative] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTnegativeb}};
\node [left=of negative] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTnegativea}};


\node (irrational) [below=of negative] {\begin{tabular}{c}
Matrix\\
$\begin{bmatrix}\cl{1\\1}&\cl{1\\-1}\end{bmatrix}$\\
Determinant\\
-2\\
Eigenvalues\\
$-\sqrt{2},\sqrt{2}$\\
Eigenvectors\\
$(1-\sqrt2,1), (1+\sqrt2,1)$
\end{tabular}};
\node [right=of irrational] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTirrationalb}};
\node [left=of irrational] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTirrationala}};

\node (complex) [below=of irrational] {\begin{tabular}{c}
Matrix\\
$\begin{bmatrix}\cl{1\\-1}&\cl{1\\1}\end{bmatrix}$\\
Determinant\\
2\\
Eigenvalues\\
$1+i,1-i$\\
Eigenvectors\\
$(-i,1), (i,1)$
\end{tabular}};
\node [right=of complex] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTcomplexb}};
\node [left=of complex] {\includegraphics[height=\myvfplotheight]{04-Linear-Transformations/support/LTcomplexa}};

\end{tikzpicture}
\caption{Four 2D transformations. See example \ref{details for four} for details. In the picture on the right, the purple vectors are eigenvectors of length 1, where the green vectors have been scaled by the eigenvalue.
}
\label{four matrix transformations}
\end{figure}
\end{example}

\marginpar{PDF documents utilize vector graphics so that you can zoom in on the image as far as you want and still have smooth edges on your text. I have created this text using vector graphics where possible so that you can zoom in on any image and still see a high quality image.} Vector graphics (such as those used in PDF documents) utilize linear transformations to create images.  
When you zoom in to view a PDF document, the computer redraws all the text so that every corner is still smooth.  
Scanners and digital cameras store their data as pixels instead of as vectors. 
If you scan a document and then zoom in, the document appears grainy.  
Vector graphics prevent pixel errors from occurring as each time you zoom in the computer knows exactly how to transform text so that smooth edges occur. 


\subsubsection{3 by 3 Matrices}

Let's now look at some visualization of 3D transformations. The ideas are similar to what happens in 2D, we just add an additional dimension. This adds another eigenvector direction, and instead of talking about an increase in area, we now talk about an increase in volume. 

\begin{example}\label{3d transformation example}
Consider the two matrices 
. Figure \ref{matrix transformation 3d example} illustrates the transformations resulting from the two matrices $\bm{ 2 & 0 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 2}$
and 
$\bm{ 0 & 0 & -1 \\ 0 & -3 & 0 \\ -2 & 0 & 0}$ in two different ways.  The first image shows how to transform a 3D surface.  The second illustration shows how to transform a sphere into an ellipsoid, and it also includes the eigenvector directions. You can log on to Sage to view more images and rotate the images to get a better view of what each transformation does. 

Both matrices have determinant 6, which means that in both cases the volume of objects is increased by 6 through the transformation.  The first transformation stretches objects by 1 in the direction $(0,-1,1)$, by 2 in the direction $(-1,0,1)$, and by 3 in the direction $(0,1,1)$.  The second transformation has two directions in which a reflection occurs (the product of two negatives results in a positive determinant).  The amount of each stretch, together with the direction, is listed in the figure.


\begin{figure}
\begin{center}
\begin{tikzpicture}
\node (middle) {\begin{tabular}{c}
Matrix\\
$\bm{ 2 & 0 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 2}$\\
Determinant\\
6\\
Eigenvalues\\
$1,2,3$\\
Eigenvectors\\
$(0,-1,1)$\\$(-1,0,1)$\\$(0,1,1)$
\end{tabular}};
\node (middle2) [below=of middle] {\begin{tabular}{c}
Matrix\\
$\bm{ 0 & 0 & -1 \\ 0 & -3 & 0 \\ -2 & 0 & 0}$\\
Determinant\\
6\\
Eigenvalues\\
$-3,-\sqrt{2},\sqrt 2$\\
Eigenvectors\\
$(0,1,0)$\\$(\sqrt2/2,0,1)$\\$(-\sqrt2/2,0,1)$
\end{tabular}};
\node  [left=of middle] {\includegraphics[width=2in]{04-Linear-Transformations/support/LT3da}};
\node  [right=of middle]  {\includegraphics[width=2in]{04-Linear-Transformations/support/LT3db}};
\node  [left=of middle2]{\includegraphics[width=2in]{04-Linear-Transformations/support/LT3d2a}};
\node  [right=of middle2] {\includegraphics[width=2in]{04-Linear-Transformations/support/LT3d2b}};
\end{tikzpicture}
\end{center}

\caption{{
Two 3D transformations. See example \ref{3d transformation example} for details. In the picture on the right, the black vectors are eigenvectors of length 1, where the purple vectors have been scaled by the eigenvalue.}}
\label{matrix transformation 3d example}
\end{figure}




\end{example}

\subsubsection{Singular Matrices - Not Invertible}

In all the examples above, we have been looking at square matrices where the determinant is nonzero.  When the determinant is zero, the matrix does not have an inverse and we say the matrix is singular.  This means that the columns are linearly dependent, which means that when we construct a visualization of the transformation, we should see multiple columns overlapping. The next example illustrates this in both 2D and 3D.  

\begin{example}
For the matrix 
$A=\bm{1&-2\\-1&2}$, we map $(1,0)$ to $(1,-1)$ and $(0,1)$ to $(-2,2)$. Both vectors lie on the same line.
We obtain the two visualizations below. \marginpar{When the determinant is zero, a 2D transformation takes the entire plane to a line. The column space is the line onto which the transformation maps.}
\begin{center}
\includegraphics[height=2in]{04-Linear-Transformations/support/LTsingular2da}
\includegraphics[height=2in]{04-Linear-Transformations/support/LTsingular2db}
\end{center}
Notice that every vector, in addition to $(1,0)$ and $(0,1)$, is mapped onto this same line.  The column space of $A$ is precisely the line given by the span of $(-1,1)$. The column space corresponds directly with the line onto which everything is mapped. Because $\det A = 0$, we know that $\lambda=0$ is an eigenvalue. The corresponding eigenvector $(2,1)$ is drawn in the image on the right. This eigenvector is found by solving $A\vec x = 0$, which means that the set of solutions is the null space. \marginpar{The null space of $A$ is precisely the set of vectors which get squashed to $\vec 0$ by $A$.} Every vector on the line containing $(2,1)$ is transformed to the origin (so in this direction we are squashing an entire line to a point).  Because this transformation squashes two dimension onto a line, it is impossible to invert the transformation (hence no inverse exists).

\end{example}
\begin{example}
For the matrix 
$A=\bm{ 0 & 1 & -1 \\ -1 & 0 & 1 \\ 1 & -1 & 0}$, we map 
the vector $(1,0,0)$ to $(0,-1,1)$, 
the vector $(0,1,0)$ to $(1,0,-1)$, and 
the vector $(0,0,1)$ to $(-1,1,0)$. These three images lie on the same plane (since the determinant is zero).  We obtain two visualizations below. \marginpar{When the determinant is zero, a 3D transformation takes the all of space to a plane (if the rank is 2) or a line (if the rank is 1). The column space is precisely the subspace of $\mathbb{R}^3$ onto which the transformation squashes things.}
\begin{center}
\includegraphics[height=2in]{04-Linear-Transformations/support/LTsingular3da}
\includegraphics[height=2in]{04-Linear-Transformations/support/LTsingular3db}
\end{center}
Notice that every vector  is mapped onto the same plane containing column vectors of $A$. 
The column space of $A$ is precisely the plane onto which everything is mapped. 
The eigenvector corresponding to $\lambda =0$ is $(1,1,1)$, found by solving $A\vec x=\vec 0$, i.e. finding the null space.
This eigenvector is drawn in the image on the right.  
Anything which lies on the line containing $(1,1,1)$ is also transformed to the origin, so in this direction we are squashing an entire line to a point. 
Because this transformation squashes three dimension onto a 2D plane, it is impossible to invert the transformation (hence $A$ has no inverse). 
Non invertible square transformations always involve squashing a dimension.

  
\end{example}

In summary, if the determinant is nonzero, then the transformation maps all of 2D to all of 2D, or all of 3D to all of 3D.  
If the determinant is zero, then the the transformation maps 2D onto a subspace of 2D (a line), or 3D onto a subspace of 3D (a plane or line).  
In all cases, the column space of the matrix is precisely the space onto which the transformation maps vectors. 
If the matrix has a zero determinant, then the column space is not all of 3D and helps us understand where to map vectors.  

\subsection{Non Square Matrices}

Every example we have looked at till now involved either a 2 by 2 or 3 by 3 matrix.  
In this case we were mapping the plane to the plane, or space to space. 
What kind of transformation does a non square matrix represent, and what use would it be?  
Every time we watch television, we are viewing a 3D world that has been represented to us in a 2D way.  
This is an example of a transformation which takes us from 3D to 2D (often written $\mathbb{R}^3\to \mathbb{R}^2$).  
Computer animators (movies and computer games) start by constructing a 3D model of a world, and then use matrix transformations to map this 3D world into a 2D presentation for viewing. 
Projecting a 2D image into 3D (written $\mathbb{R}^2\to \mathbb{R}^3$) requires a transformation from 2D to 3D. 

How does the size of the matrix determine the kind of transformation.  
For an $m$ by $n$ matrix $A$, the matrix product $A\vec x = \vec b$ requires that $\vec x$ be $n$ dimensional, and $\vec b$ be $m$ dimensional.  
So when you put in vectors of size $n$, you get out vectors of size $m$.  
This meas that a matrix $A_{mn}$ results in a linear transformation $A:\mathbb{R}^n\to\mathbb{R}^m$. 
Did you notice the change in order from $mn$ to $n\to m$? 
The size of the matrix tells us first the dimension of the outputs and then the dimension of the inputs.
Let's look at two more examples.  
Then we'll be ready to generalize the patterns we've observed into a general theory about linear transformations.  

\begin{example}
Let's look at the two transformations given by the matrices
$A=\bm{0&1\\1&0\\1&1}$ and 
$B=\bm{-2&-1&-1\\1&2&-1}$.
The first transformation requires that we input 2D vectors and returns 3D vectors. To graph this transformation, we notice that multiplying $A$ by the vectors $(1,0)$ and $(0,1)$ returns the two columns of $A$.  So in the $xy$ plane we draw the unit square, and in 3D we graph the parallelogram formed by the two columns.  This is illustrated below. Notice that the unit circle becomes an ellipse in the plane spanned by the columns of $A$.  Again we see that all the output vectors are drawn in the column space of $A$.
\begin{center}
\includegraphics[width=\marginparwidth]{04-Linear-Transformations/support/LT2dto3d}
\end{center}

The transformation given by $B$ requires that we input a 3D vector to obtain a 2D vector.  We graph the unit cube in 3D and graph the columns of $A$ in the $xy$ plane. In this transformation, we are squashing a dimension out of the problem. This is the kind of transformation used to cast shadows in 3D graphics, as well as to present a 2D image of 3D objects for viewing on a screen. 
\begin{center}
\includegraphics[height=2in]{04-Linear-Transformations/support/LT3dto2d}
\end{center}


\end{example}






\section{What is a Linear Transformation?}

\subsection{General Function Notation and Vocabulary}
Let's start with a review of function notation from beginning algebra. 
A function $y=f(x)$ is a rule which assigns to each input $x$ from the domain a value $y$ from the range. 
Notationally we may write $f:D\to R$ to represent the function as a map from $D$ (the domain) to $R$ (the range).  
The sets $D$ and $R$ in previous classes have normally been the set of real numbers. 
Before we proceed to work with functions between vector spaces, we need to build some vocabulary. 
We now consider functions (maps or transformations) from any set $D$ to any set $R$.
\begin{definition}
\begin{itemize}
	\item Any mapping which assigns to each element of $D$ exactly one element of $R$ is called a function. We write $f:D\to R$ to emphasis the domain $D$ and range $R$ (also called the codomain). 

	\item The image of an element $x\in D$ is the value $y\in R$ such that $f(x)=y$. We can write $x\mapsto y$ (read $x$ maps to $y$) to illustrate the idea that the input $x$ transforms into the output $y$ through this function. 

	\item The image of $f$, written $\im f=f(D)$, is the set of $y\in R$ which are the image of some $x\in D$. In set notation we write $\im f = \{y\in R\ |\ f(x)=y$ for some $x\in D\}$.

	\item The preimage of $y\in R$, written $f^{-1}(y)$, is the set of all $x\in D$ such that $f(x)=y$. 
	
	\item We say a function is one-to-one, or injective, if $f(x_1)=f(x_2)$ implies that $x_1=x_2$ (every $y$ value in the image has only one $x$ value which maps to it).  
	\item We say a function is onto, or surjective, if every $y\in R$ is the image of some $x\in D$ (the function hits every $y$ value in the range).  
	\item We say a function is bijective if $f$ is both one-to-one and onto (both injective and surjective). In this case we define $f^{-1}$ to be the function which inverts $f$. 
	\item The composition of two functions $f:A\to B$ and $g:B\to C$ is a new map $g\circ f:A\to C$ defined by $(g\circ f)=g(f(a))$. Notice that in order to compose two maps, the image of $f$ must be a subset of the domain of $g$.  

\end{itemize}
\end{definition}


\begin{example}
Consider the sets $D=\{A,B,C\}, R_1=\{1,3,9\},$ and $R_2=\{1,3,7,9\}$.
\begin{enumerate}
	\item If we define $f(A)=1,f(B)=9,f(C)=3$, then $f:D\to R_1$ is a function.  The image of $B$ is $9$, so we write $B\mapsto 9$. The image of $f$ is $f(D)=\{1,3,9\}$, so $f$ is onto. The preimage of $3$ is $f^{-1}(3)=C$. This map $f:D\to R_1$ is injective and surjective, hence it has an inverse. However if I change the range to $R_2$ instead of $R_1$, then the function is not surjective (onto) because 7 is not the image of any value in $D$ (the preimage of $7$ is empty).
	\item If we define $f(A)=1,f(A)=9,f(C)=3$, then $f:D\to R_1$ is not a function. For one, $f(B)$ is not defined. In addition, $f(A)$ takes on two values, $A\mapsto 1,9$ which is not allowed.
	\item If we define $f(A)=1,f(B)=3,f(C)=3$, then $f$ is a function.  The image of both $B$ and $C$ is $9$ so the preimage of $3$ is $f^{-1}(3)=\{B,C\}$, which means the map is not injective (as $f(B)=f(C)$, but $B\neq C$).
	\item If we define $f:A\to R_1$ by $f(A)=1,f(B)=3,f(C)=9$, and we define $g:R_1\to R_2$ by $g(1),g(3)=3,g(9)=7$, then the composite function $g\circ f$ satisfies $g(f(A))=g(1)=1$, $g(f(B))=g(3)=3$, and $g(f(C))=g(9)=7$. The composite is injective, but not surjective (it misses 9). 
\end{enumerate}
\end{example}

\subsection{Linear Transformations}
One of the main topics in linear algebra is the study of special types of functions, called linear transformations (or linear mappings) where the domain and codomain (range) are both vector spaces.  All of the transformations resulting from matrices in the first section are linear transformations.  The following definition is a result of noticing the patterns which arose from the examples above.  
\begin{definition}[Linear Transformation]
We call a function $T:V\to W$ between vector spaces $V$ and $W$ linear (or called a linear transformation) if both of the following criteria are satisfied:
\begin{enumerate}
	\item $T(\vec v_1+\vec v_2)=T(\vec v_1)+T(\vec v_2)$ for every $\vec v_1,\vec v_2\in V$
	\item $T(c\vec v)=cT(\vec v)$ for every scalar $c$ and vector $\vec v\in V$.
\end{enumerate}
We say that $T$ preserves vector addition and scalar multiplication.
\end{definition}
\marginpar{If $T$ is linear, then the transform of a linear combination is the linear combination of the transforms: $$T\left(\sum_{i=1}^n c_i\vec v_i\right) = \sum_{i=1}^n T(c_i\vec v_i).$$}
A linear transformation is a function between vector spaces in which you can perform vector addition and scalar multiplication  either before or after applying the function. 
If a function is linear, then repeated application of the definition shows that $$T(c_1\vec v_1+\cdots +c_n\vec v_n) = c_1T(\vec v_1)+\cdots +c_nT(\vec v_n).$$ Linear combinations of vectors becomes linear combinations of the images of each vector when using a linear transformation.  \marginpar{Once you know what a linear transformation does to a basis, you know what it does to all of the space.}

 In particular, this means that if you know the value of the linear transformation at each point of a basis for $V$, then every other value of the function is known as well by writing every other vector as a linear combination of basis vectors. 
\begin{example}
Let's start by illustrating what a linear transformation is not. We'll show that the function $f(x) = 2x+1$, $g(x)=x^2$, and $h(x,y)=(x+y,xy)$ are not linear transformations.
\begin{enumerate}
	\item The function $f(x)=2x+1$ (whose graph is a line which does not pass through the origin) is not linear because
	 $$f(x+y)=2(x+y)+1 = 2x+2y+1$$ whereas if we transform each point and then sum we obtain $$f(x)+f(y)=2x+1+2y+1 = 2x+2y+2.$$
	 Because $f(x+y)\neq f(x)+f(y)$, this function is not linear. 
	 \marginpar{Linear transformations map $\vec 0$ to $\vec 0$.}
	 Linear transformations always map the zero vector to the zero vectors (lines through the origin are linear transformations).
	\item The function $g(x)=x^2$ (a parabola) is not linear because $$g(x+y)=(x+y)^2 = x^2+2xy+y^2 = g(x)+g(y)+2xy \neq g(x)+g(y).$$ 
	Alternatively, we could also look at scalar multiplication to show this function is not linear, because $g(2x) = 4x^2$ whereas $2g(x) = 2x^2$ and so $g(2x)\neq 2g(x)$. 
	Raising variables to powers other than 1 results in nonlinear maps.
	\item The function $h(x,y)=(x+y,xy)$ is not linear. We can show this by looking at an example, rather than using variables. 
	We compute $f(1,1)=(2,1)$ and $f(2(1,1)) = f(2,2)=(4,4)$, but $2f(1,1) = (4,2)$. 
	So we have shown by example that $f(2\vec x)\neq 2f(\vec x)$. 
	\marginpar{You can show a function is not linear by providing one example.}
	The map does not preserve scalar multiplication. 
	The component $xy$ is the problem here. In general, multiplying variables together creates nonlinear functions.  
\end{enumerate}
\end{example}

Now let's look at some examples of linear transformations. 
In each example, we will formally show that the function is linear by showing that it preserves both vector addition and scalar multiplication.  
The first 3 examples are crucial, while the last 3 represent ideas explored more in future classes where vector spaces of infinite dimension are studied. 
Practical applications to the telecommunications industry, jpeg, mp3, and many more require knowledge about infinite dimensional vector spaces.  
In this class we will focus on transformations between finite dimensional vector spaces.

\begin{example}
\marginpar{Lines through origin $y=mx$ are linear transformations from $\mathbb{R}$ to $\mathbb{R}$.}
The function $T(x)=4x$ is linear because (1) $T(x+y) = 4(x+y) = 4x+4y = T(x)+T(y)$ (the map preserves vector addition) and (2) $T(cx)=4(cx)=c(4x)=cT(x)$ (the map preserves scalar multiplication).
\end{example}

The first example showed that any line through the origin, $y=mx$, is a linear transformation. In the next two examples, we'll show that any function of the form $\vec y = M\vec x$ is a linear transformation as well.  The reason we use the letter $m$ to represent slope is because the $m$ stands for ``matrix,'' where we replace $x$ and $y$ with vectors. Linear transformations are really just extensions of the simple line $y=mx$ to higher dimensions. The slope becomes a matrix, while the variables become vectors.

\begin{example} \label{ltex matrix1}
The function $T:\mathbb{R}^2\to \mathbb{R}^2$ defined by $T(x,y)=(2x+3y,x-y)$ is linear.  To show this, we check that $T$ preserves both vector addition and scalar multiplication.
\begin{enumerate}
	\item $T((x,y)+(s,t)) = T((x+s,y+t))=(2(x+s)+3(y+t),(x+s)-(y+t)) = (2x+3y,x-y)+ (2s+3t,s-t) = T(x,y)+T(s,t)$, hence the map preserves vector addition. 
	\item $T(c(x,y))= T(cx,cy) = (2cx+3cy,cx-cy)=c(2x+3y,x-y)=cT(x,y)$, hence the map preserves scalar multiplication.
\end{enumerate}
 We can write this function in matrix form as 
	$$T(x,y)=(2x+3y,x-y)=\begin{bmatrix}2&3\\ 1&-1\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}.$$ 	
	\marginpar{The columns of the matrix are the images of the basis vectors (1,0) and (0,1). }
	Notice that $T(1,0)=(2,1)$ is the first column and $T(0,1)=(3,-1)$ is the second column.

\end{example}

\begin{example}\label{ltex matrix2}
Now let's work with a vector space other than $\mathbb{R}^n$. 
Let $V=P_2(x)$ be the set of polynomials of degree 2 or less, together with the zero polynomial.  
The derivative function $D:P_2(x)\to P_2(x)$ defined by $D(a+bx+cx^2)=b+2cx$ is a linear transformation. 
\begin{enumerate}
	\item The derivative preserves vector addition because $(f+g)'=f'+g'$ by the sum rule for derivatives.
	\item The derivative preserves scalar multiplication becuse $(cf)'=c(f)'$ by the constant multiple rule for derivatives.
\end{enumerate}
The vectors $\{1,x,x^2\}$ are the standard basis vectors for $P_2(x)$. 
Relative to this standard basis, the coordinates of $a+bx+cx^2 = a(1,0,0)+b(0,1,0)+c(0,0,1)$ are just $(a,b,c)$. 
We can then rewrite the derivative transformation in coordinate form as $D(a,b,c) = (b,2c,0)$. 
In matrix form, we write
 $$D(a,b,c)
 =(b,2c,0)
 =\begin{bmatrix}0&1&0\\0&0&2\\0&0&0\end{bmatrix}\begin{bmatrix}a\\b\\c\end{bmatrix}.$$ 
 Notice that $D(1,0,0)=(0,0,0)$ is the first column, $D(0,1,0)=(1,0,0)$ is the second column, and $D(0,0,1)=(0,2,0)$ is the third column. 
 \marginpar{The columns of our matrix are the coordinates of the images of our basis vectors}
 In general, the columns of our matrix will be the coordinates of the images of the standard basis vectors.
\end{example}

\begin{example}
Let $V=P(x)$ be the set of polynomials, an infinite dimensional vector space with basis $\{1,x,x^2,x^3,\ldots\}$.  
The derivative transformation $D:P(x)\to P(x)$ is a linear transformation because of the sum rule and constant multiple rule for derivatives (just as in the last example). 
For those of you who have had math 113, Taylor polynomials and Taylor series are all related to this vector space and transformations on it.
\end{example}

\begin{example}
Let $V$ be the set of functions that are infinitely differentiable on the entire real number line (so $e^x, \sin x, \cos x, x^2$, etc. are examples). 
The derivative transformation $D:V\to V$ defined by $D(f) = f'$ is a linear transformation on this space (by the same arguments as the previous example). Notice that you input a function $f$ and get out a function $f'$, so this map takes vectors in $V$ to vectors in $V$.  This space is infinite dimensional, and a basis for this space is so large that we say it is uncountable (those of you headed on to more math classes will study the difference between countably infinite and uncountably infinite in future classes).

Let $T:V\to \mathbb{R}$ be the integral transformation, defined by $T(f)=\int_0^1 f(x)dx$.  This transformation takes a function $f$ and returns a real number by integrating from $[0,1]$.  To show this function is linear, we check 
\begin{enumerate}
	\item $\ds \int_0^1 f(x)+g(x)dx=\int_0^1 f(x)dx+\int_0^1 g(x)dx$, so $T(f+g)= T(f)+T(g)$ (vector addition is preserved), 
	\item $\ds \int_0^1 cf(x)dx =c\int_0^1 f(x)dx$, so $T(cf) = cT(f)$ (scalar multipilcation is preserved).
\end{enumerate}
The derivative and integral are both examples of linear transformations that you have studied before. The difference is that the derivative returns functions, while the integral returns numbers.  The codomain is $V$ for derivatives, and the vector space $\mathbb{R}$ for integrals.  
In real analysis, these ideas are explored in greater depth and become foundational tools for understanding finance, economics, and more.
\end{example}

\begin{example}
Let $V$ be the set of infinitely differentiable function on the real line.  Let $$L(y(x))=y^{\prime\prime}(x)-2xy^\prime(x)+3x^2 y(x)$$ be the differential operator which takes multiple derivatives of $y$ and combines them together by multiplying by functions of $x$.  This is a linear transformation $L:V\to V$ because 
\begin{enumerate}
	\item $L(y_1+y_2)=(y_1+y_2)^{\prime\prime}-2x(y_1+y_2)^\prime+3x^2 (y_1+y_2) = (y_1^{\prime\prime}-2xy_1^\prime+3x^2 y_1) +(y_2^{\prime\prime}-2xy_2^\prime+3x^2 y_2)=L(y_1)+L(y_2)$, and
	\item $T(cy)=(cy)^{\prime\prime}-2x(cy)^\prime+3x^2 (cy) = c(y^{\prime\prime}-2xy^\prime+3x^2 y)=cT(y)$.
\end{enumerate}
 We use transformations like $L$ to find solutions to differential equations, and show that the set of solutions is a vector space. These ideas are studied more in differential equations, engineering, and physics, and become the foundational tool for understanding mechanical systems, electrical circuits, radio waves, signal processing, and more.  
\end{example}






\subsection{Standard Matrix Representation}
Every matrix we have studied this semester represents a linear transformation. 
To verify that every matrix represents a linear transformation, we need to show that matrices preserve vector addition and scalar multiplication. We have already seen that $A(\vec x+\vec y) = A\vec x+A\vec y$ and $A(c\vec x)=c(A\vec x)$, which is precisely what is meant by matrix multiplication preserves vector addition and scalar multiplication.  

Conversely, every linear transformation (between finite dimensional vector spaces) can be expressed in matrix form $T(\vec x)=A\vec x$ for some matrix $A$.
Examples \ref{ltex matrix1} and \ref{ltex matrix2} illustrate how to find the matrix $A$ relative to the standard basis. . 
To find this matrix, we start by computing the image of each standard basis vector, i.e. $$T(1,0,\ldots,0),T(0,1,\ldots,0), \ldots,T(0,0,\ldots,1).$$ We then place these vectors in the columns of a matrix.  This matrix $A$ is called the standard matrix representation of the linear transformation. We use the word ``standard'' because are using the standard basis vectors to give the coordinates of each vector.  The next two units explore using different basis vectors to describe linear transformations. Time for some examples.

\begin{example}
Let's find the standard matrix representation of the linear transformation $$T(x,y,z,w)=(2x+3y-4z,x+y-w,z+2x-5w).$$ We start by computing the values at each of the 4 basis vectors  $$T(1,0,0,0)=(2,1,2),T(0,1,0,0)=(3,1,0),T(0,0,1,0)=(-4,0,1),T(0,0,0,1)=(0,-1,-5).$$ 
We now place these vectors into the columns of a matrix which gives the standard matrix representation
$$T(x,y,z,w)=A\vec x=
\begin{bmatrix}
2&3&-4&0\\
1&1&0 &-1\\
2&0&1 &-5
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
z\\
w
\end{bmatrix}.$$ 
The matrix $A$ is called the standard matrix representation of $T$. 
Notice that we input 4D vectors (hence 4 columns) and get out 3D vectors (hence 3 rows).  
\marginpar{The number of rows is the dimension of the range. The number of columns is the dimension of the domain.} 
Each input dimension gets a column, and each output dimension gets a row.   
\end{example}

Sometimes a linear transformation is described in words.  Examples include things like ``rotate everything 30 degrees'' or ``find the shadow of this object if the sun is located at ...''  In such a case, you can create the standard matrix representation of the transformation by asking where each basis vector is sent.  Here is an example.

\begin{example}
Let's rotate an object in the plane counterclockwise by 90 degrees. 
How do we obtain the standard matrix representation of this transformation? 
We find what happens to the standard basis vectors. 
We'll let $T(x,y)$ be the linear transformation which rotates counterclockwise by 90 degrees, we see that $T(1,0)=(0,1)$ and $T(0,1)=(-1,0)$.  
This means the standard matrix representation of $T$ is 
$$\begin{bmatrix}
\cos \pi/2&\cos\pi\\
\sin\pi/2&\sin\pi
\end{bmatrix}=\begin{bmatrix}0&-1\\1&0\end{bmatrix}. \quad\quad \text{(a rotation of 90 degrees)}$$

If instead we want to rotate counterclockwise by 30 degrees ($\pi/6$ radians)  then we compute 
$T(1,0) = (\cos \pi/6, \sin \pi/6) = (\sqrt3/2,1/2)$ (the values on the unit circle at $\pi/6$)  and 
$T(0,1) = (\cos 2\pi/3, \sin 2\pi/3) = (-1/2,\sqrt3/2) $ (the values on the unit circle at $\pi/2+\pi/6 = 2\pi/3$). 
This means the standard matrix representation of $T$ is 
\marginpar{
\includegraphics[width=\marginparwidth]{04-Linear-Transformations/support/LTrotate30}

A 30 degree rotation.}
$$
\begin{bmatrix}
\cos \pi/6&\cos(\pi/2+\pi/6)\\
\sin\pi/6&\sin(\pi/2+\pi/6)
\end{bmatrix}
=\begin{bmatrix}\sqrt3/2&1/2\\-1/2&\sqrt3/2\end{bmatrix}. \quad\quad \text{(a rotation of 30 degrees)}$$  

\end{example}

Let's generalize the pattern we've seen above to include rotations through any angle. We can write the image of the second basis vector above in terms of trig function at $\theta$, instead of at $\pi/2+\theta$, if we recall that $\cos(\pi/2+\theta) = -\sin(theta)$ and $\sin(\pi/2+\theta) = \cos(theta)$ (these identities just say that shifting cosine left $\pi/2$ gives negative sine, and shifting sine left $\pi/2$ gives cosine.  
We ca now write the standard matrix representation of a rotation through $\theta$ degrees as
$$
\begin{bmatrix}
\cos \theta&\cos(\pi/2+\theta)\\
\sin \theta&\sin(\pi/2+\theta)
\end{bmatrix}
=
\begin{bmatrix}
\cos \theta&-\sin\theta\\
\sin \theta&\cos\theta
\end{bmatrix}.
\quad\quad \text{(a rotation of $\theta$ radians)}
$$




Sometimes we are able to extract from a problem information about how a transformation changes vectors other than the standard basis vectors.  When this occurs, provided we know that the transformation does to a basis, we can still find the standard representation. It just requires a little more work. Let's look at an example. 

\begin{example}
Suppose we know that $T(x,y)$ is a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^3$ and we know that $T(1,1)=(3,0,5)$ and $T(2,0)=(1,-2,0)$. Because we haven't computed the transformation at $(1,0)$ and $(0,1)$
We don't know $A$ yet, but we do know that 
$$
A\begin{bmatrix}1\\1\end{bmatrix} = \begin{bmatrix}3\\0\\5\end{bmatrix}, 
A\begin{bmatrix}2\\0\end{bmatrix} = \begin{bmatrix}1\\-2\\0\end{bmatrix},\quad \text{ or  } \quad
A\begin{bmatrix}1&2\\1&0\end{bmatrix} = \begin{bmatrix}3&1\\0&-2\\5&0\end{bmatrix}
$$  
The vectors $(1,1)$ and $(2,0)$ are linearly independent, so they form a basis for $\mathbb{R}^2$. The matrix formed by placing these vectors in columns, $\begin{bmatrix}1&2\\1&0\end{bmatrix}$, is an invertible matrix since the columns are independent. Since it is invertible, we can multiply both sides of our equation on the right by this inverse to obtain $$A= \begin{bmatrix}3&1\\0&-2\\5&0\end{bmatrix}\begin{bmatrix}1&2\\1&0\end{bmatrix}^{-1} = \begin{bmatrix}3&1\\0&-2\\5&0\end{bmatrix}\begin{bmatrix}0&1\\1/2&-1/2\end{bmatrix} =\begin{bmatrix}1/2&5/2\\-1&1\\0&5\end{bmatrix}.$$ 
We found the standard matrix representation by using the inverse of basis vectors given. We can check that our matrix $A$ is correct by computing the matrix product 
$$\begin{bmatrix}1/2&5/2\\-1&1\\0&5\end{bmatrix}
\begin{bmatrix}1&2\\1&0\end{bmatrix} 
= \begin{bmatrix}3&1\\0&-2\\5&0\end{bmatrix}$$ to show that $T(1,1)=(3,0,5)$ and $T(2,0)=(1,-2,0)$ as required.  We also know now that $T(1,0) = (1/2,-1,0)$ and $T(0,1)=(5/2,1,5)$, so we know the images of the standard basis vectors. 
\end{example}
 
In general, when you know what a transformation does to any basis of the domain, you can use this to find the standard matrix by multiplying on the right by an inverse.  
Schaum's outlines writes out the equations and then solves them (which is equivalent to using an inverse). 
Let's focus on the fact that the inverse is the key tool needed to understand the transformation.




  
\section{Subspaces From Linear Transformations}

In the first section where we graphed matrix transformations, we noticed that the column space of the matrix was related to the possible outputs of the transformation. We also saw that the null space of the matrix told us precisely which vectors in the domain were mapped to the origin.  These two spaces associated with matrices are important subspaces related to general linear transformations.  Because a linear transformation could have a domain or range which is infinite dimensional, we need a new word to talk about these subspaces of transformations, even when no matrix exists to represent the transformation.  The new words are image and kernel. Here are the formal definitions.

\begin{definition}
Let $T:V\to W$ be a linear transformation.  
\begin{enumerate}
	\item The image of $T$ (written $\im T$) is the set of all values in $W$ which are the image of some $\vec v$ in $V$. 
	Symbolically we write $$\im T = \{\vec y\in W \ | \ T(\vec x)=\vec y \text{ for some $\vec x\in V$}\}.$$ 
	\item The kernel of $T$ (written $\ker T$) is the set of all values in $W$ whose image is $\vec 0$ in $W$. 
	Symbolically we write $$\ker T = \{\vec x\in V \ | \ T(\vec x)=\vec 0\}.$$
\end{enumerate}
\end{definition}

Linear transformations may not always be represented with matrices. However, when you can represent a transformation with a matrix, these new words are precisely the same as the column space and null space of a matrix.  Let's formalize this pattern into a theorem.
\begin{theorem}
If $T:V\to W$ is a linear transformation between finite dimensional vector space, and the standard matrix representation of $T$ is $A$, 
\begin{enumerate}
	\item the column space of $A$ equals the image of $T$ and 
	\item the null space of $A$ equals the kernel of $T$.
\end{enumerate}
\end{theorem}

We already know that the column space and null space of a matrix are vector subspaces.  This should also be true of the image and kernel of the transformation. Remember, to show that something is a subspace we must show that it contains $\vec 0$ and is closed under vector addition and scalar multiplication.  Let's first state our next theorem, and then verify that it is indeed true. 

\begin{theorem}
Let $T:V\to W$ be a linear transformation. 
\begin{enumerate}
	\item The image of $T$ is a vector subspace of the range (codomain) $W$. 
	\item The kernel of $T$ is a vector subspace of domain $V$.
\end{enumerate}
\end{theorem}
Notice that the image is a subspace of the range, and the kernel is  subspace of the domain. The image is the set of outputs, and the kernel is the set of inputs that map to zero. 

\begin{example}
Let's show that the image of $T$ is a vector subspace of $W$. I'll let you show that the kernel is a vector subspace of $V$ in the homework.

We have to show three things: (1) the zero vector in $W$ is in the image, (2) the image is closed under addition, and (3) the image is closed under scalar multiplication.
\begin{enumerate}
	\item We know that linear transformations map the zero vector to the zero vector.  So the image of the zero vector $\vec 0_V$ in $V$ is precisely $T(\vec 0_V) = \vec 0_W$, the zero vector in $W$.  Hence $\vec 0_W$ is in the image.
	\item We need to show that if $\vec y_1$ and $\vec y_2$ are in the image, then so is their sum.  
	Well, since $\vec y_1$ belongs to $W$, we know that $\vec y_1 = T(\vec x_1)$ for some $\vec x_1\in V$.  
	Similarly $\vec y_2 = T(\vec x_2)$ for some $\vec x_2\in V$.  
	We can now compute (since $T$ is linear and preserves addition) \marginpar{Linear transformations preserve addition. This is why the image is closed under addition.}  $$T(\vec x_1+\vec x_2) = T(\vec x_1)+T(\vec x_2) = \vec y_1+\vec y_2$$ 
	which means that the sum $\vec y_1+\vec y_2$ is the image of $\vec x_1+\vec x_2$. 
	This shows that the image is closed under vector addition.
	\item The only thing that remains is to show that if $\vec y$ is in $W$, then so is any scalar multiple of $\vec y$. Since $\vec y\in W$, we know there is some $\vec x\in V$ such that $T(\vec x)=\vec y$. 
	If $c$ is any constant, then we can compute (since $T$ is linear and preserves scalar multiplication) 
	\marginpar{Linear transformations preserve scalar multiplication. This is why the image is closed under scalar multiplication.}
	$$T(c\vec x) = cT(\vec x) = c\vec y$$ which means that $c\vec y$ is the image of $c\vec x$.
	This shows that the image is closed under scalar multiplication.
	
\end{enumerate}

Your job in the homework will be to show that the kernel of a linear transformation is a vector subspace of the domain $V$.

\end{example}

Now that we know the image and kernel of a transformation are vector subspaces, we can look for spanning sets, bases, and find the dimension of these spaces. We used the word ``rank'' as the dimension of the column space, but have not yet given a name to the dimension of the null space.  Here are some more definitions.

\begin{definition} 
Let $T:V\to W$ be a linear transformation. 
The dimension of the image of $T$ is called the rank of $T$ (just as the rank is the dimension of the column space).
The dimension of the kernel of $T$ is called the nullity of $T$ (and we use the same word for the null space). 

\end{definition}

Now that we have some words to describe the dimensions of the image and kernel, we can state one of the key theorems which relates these two subspaces.  A basis for the column space of a matrix $A$ is simply the pivot columns of $A$. We find a basis for the null space by solving for all the variables in terms of the free variables, which means that each free variable contributes a vector to the basis of the null space.  Since the free variables correspond to non pivot columns, and the column space corresponds to pivot columns, we see that the sum of the dimensions of the column space and null space must always equal the number of columns.  

\begin{theorem}[The Rank-Nullity Theorem]
Let $T:V\to W$ be a linear transformation between finite dimensional vector spaces, whose standard matrix representation is $A$.  Then 
\begin{enumerate}
	\item The rank of $T$ is the number of pivot columns of $A$. 
	\item The nullity of $T$ is the number of non pivot columns of $A$. 
	\item The rank of $T$ plus the nullity of $T$ is always equal to the dimension of $V$. 
\end{enumerate}
\end{theorem}

Notice that the third item does not refer at all to the matrix $A$.  The third fact is true provide that only $V$ is finite dimensional (and hence has no standard matrix representation). Time for some examples.


\begin{example}
Consider the linear transformation $T:{\mathbb{R}}^5\to {\mathbb{R}}^6$ with standard matrix 
$$A=\begin{bmatrix}
 1 & 2 & 0 & 1 & 1 \\
 2 & -3 & -7 & -1 & 13 \\
 0 & 1 & 1 & 1 & -1 \\
 4 & 0 & -8 & 4 & 20 \\
 3 & 0 & -6 & 3 & 15 \\
 0 & 0 & 0 & 0 & 0
 \end{bmatrix} 
\xrightarrow{rref}
\begin{bmatrix}
 1 & 0 & -2 & 0 & 4 \\
 0 & 1 & 1 & 0 & -2 \\
 0 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0
\end{bmatrix}.$$

The column space of $A$ is the span of the columns of $A$. 
The image $T$ is all vectors of the form $A\vec x$ - but this is the span of the columns of $A$. 
This means that the column space of $A$ and the image of $T$ are the same vector subspace of ${\mathbb{R}}^6$, the range of $T$. 
A basis for this vector subspace is the set of pivot columns $$\{(1, 2, 0, 4, 3, 0), (2, -3, 1, 0, 0, 0), (1, -1, 1, 4, 3, 0)\},$$ so the column space has dimension 3. The rank of $A$ and the rank of $T$ are both 3.

To find the null space of $A$, and hence the kernel of $T$, we solve the equation $A\vec x=0$.  The 3rd and 5th columns correspond to the free variables.  We can write the solution as 
$$
\begin{matrix}
x_1-2x_3+4x_5=0\\
x_2+1x_3-2x_5=0\\
x_3=x_3\\
x_4+x_5=0\\
x_5=x_5
\end{matrix}
\Rightarrow
\begin{matrix}
x_1=2x_3-4x_5\\
x_2=-1x_3+2x_5\\
x_3=x_3\\
x_4=-x_5\\
x_5=x_5
\end{matrix}
\Rightarrow
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5
\end{bmatrix}=
x_3
\begin{bmatrix}
2\\
-1\\
1\\
0\\
0
\end{bmatrix}+
x_5
\begin{bmatrix}
-4\\
2\\
0\\
-1\\
1
\end{bmatrix}.
$$
We see that the null space is the span of the vectors $$\{(2,-1,1,0,0),(-4,2,0,-1,1)\}.$$ 
The kernel of $T$ is a vector subspace of ${\mathbb{R}}^5$, the domain of $T$.  
The numbers in these vectors are the opposite of the numbers in the non pivot columns of rref, with a 1 or 0 placed in the spots representing the free variables.  
The reason we negate all the numbers is that we have to subtract these numbers from both sides of the first equation to solve for each variable in terms of the the free variables.  
The nullity of $T$ is 2 (the number of free variables). 
The rank plus the nullity equals $3+2=5$, the dimension of the domain (or number of columns).
\end{example}


%The null space of a transformation can help us understand how the transformation modifies space.  If the null space is empty, then no vectors will be mapped to zero except for zero itself.  This means that the transformation will map lines to line, planes to planes, 3D spaces to 3D spaces, etc.  In this case, we say the linear transformation is nonsingular. If the domain and range have the same dimension, then the transformation will have an inverse. If however the transformation has a nonzero kernel (null space is not just zero), then in some direction things the dimension is reduced. We call the transformation singular, and it cannot have an inverse. A entire line gets mapped to a point, or an entire plane gets mapped to a line.  The dimension of the kernel helps us understand how much ``squashing'' is happening with our transformation. Use the Maple illustration code to see what happens with some transformations which are singular.

\subsection{Solutions to Systems and the Kernel}

The kernel arises from solving the equation $T(\vec x)=\vec 0$.  Can we use the kernel to help us solve the equation $T(\vec x)=\vec b$?  The answer is amazingly yes. The key lies in the fact that if both $\vec x_1$ and $\vec x_2$ are solutions to $T(\vec x)=\vec b$, then their difference $\vec x_1-\vec x_2$ is in the kernel of $T$.  This follows immediately from the computation $T(\vec x_1-\vec x_2)=T(\vec x_1)-T(\vec x_2)=\vec b-\vec b=\vec 0$. This means that $\vec x_2=\vec x_2+\vec x_h$ where $\vec x_h$ is some vector in the kernel of $T$. Here is the general idea.
\begin{theorem}
Let $T$ be a linear transformation. Consider the nonhomogeneous equation $T(\vec x)=\vec b$.
If $\vec x_p$ is one particular solution to the equation $T(\vec x)=\vec b$, then every solution can be written in the form $\vec x = \vec x_p+\vec x_h$, where $\vec x_h$ is a solution to the corresponding homogeneous equation $T(\vec x)=\vec 0$.  In other words, every solution to the nonhomogeneous equation is found by first finding a particular solution $\vec x_p$, and then adding to it every vector in the kernel.  
\end{theorem} 
This theorem is a key tool used to solve differential equations.  Once you know one solution, you can find every other solution by just adding to that solution the entire kernel (null space). This makes the kernel an extremely useful subspace to know.

\begin{example}
Suppose that a system of equations reduces to the matrix below.
\begin{center}
\begin{tabular}{cc}
$\begin{bmatrix}[ccccc|c] 0&1&0&2&0&0\\0&0&1&3&0&1\\0&0&0&0&1&4\\0&0&0&0&0&0\end{bmatrix}$
&
$\begin{bmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{bmatrix} 
= \begin{bmatrix}0\\0\\1\\0\\4\end{bmatrix}
+x_1\begin{bmatrix}1\\0\\0\\0\\0\end{bmatrix}
+x_4\begin{bmatrix}0\\-2\\-3\\1\\0\end{bmatrix}$
\end{tabular}
\end{center} 
A particular solution to this system is $\vec x_p=(0,0,1,0,4)$ (obtained by letting the free variables $x_1$ and $x_4$ both be zero).  The null space is the span of the vectors $(1,0,0,0,0)$ and $(0,-2,-3,1,0)$. Notice that every solution to the system can be written in the form $\vec x_p +\vec x_h$, where $\vec x_h$ is a vector in the null space.
\end{example}
 
\subsection{Eigenspaces are Null Spaces}
We have already notice in the first section that the eigenvectors of a matrix tell us the directions in which a transformation pushes points radially outwards.  These directions provide us a line (vector subspace) in which all transforming is done radially outwards (or inwards if the eigenvalues are less than one in magnitude).  Recall that corresponding to each eigenvalue is an infinite collection of eigenvectors. By definition, the zero vector is not an eigenvector, hence the set of eigenvectors cannot be a vector subspace.  However, if we include the zero vector, we obtain a vector space. We now make some formal definitions.

\begin{definition}[Eigenspaces]
If $\lambda$ is an eigenvalue of a matrix $A$, then the set of vectors $E_\lambda = \{\vec x | \ A\vec x = \lambda \vec x\}$ (the eigenvectors together with zero) is called the eigenspace of $A$ corresponding to $\lambda$. 

We say that $\lambda$ is an eigenvalue of a linear transformation $T:V\to V$ (notice the domain and range are the same) if 
$T(\vec x)=\lambda \vec x$ for some nonzero $\vec x$, called an eigenvector of $T$ corresponding to $\lambda$. 
If $\lambda$ is an eigenvalue of $T$, then the set of vectors $E_\lambda = \{\vec x | \ T(\vec x) = \lambda \vec x\}$ (the eigenvectors together with zero) is called the eigenspace of $T$ corresponding to $\lambda$. 
\end{definition}
We find eigenvectors of a matrix by solving $(A-\lambda I)\vec x = \vec 0$ which means that the eigenspace $E_\lambda$ is precisely the null space of $(A-\lambda I)$.  Because the eigenspace is a null space, we know that it is a vector space.

%Another important subspace which occurs when considering a square matrix is the eigenspace of an eigenvector.  If we pick a single eigenvalue $\lambda$ and add $\vec 0$ to the set of eigenvectors, then this space is a subspace of the domain. This is because if $\vec x_1$ and $\vec x_2$ are both eigenvectors, then $A(\vec x_1+\vec x_2) = A\vec x_1+A\vec x_2 = \lambda\vec x_1+\lambda\vec x_2 = \lambda(\vec x_1+\vec x_2)$, which means $\vec x_1+\vec x_2$ is an eigenvector as well.  Similarly, $A(c\vec x_1) = c(\lambda\vec x_1)=\lambda (c\vec x_1)$, which means $c\vec x_1$ is an eigenvector as well. This shows that each eigenspace of $A$ (one for each eigenvalue) is a vector subspace of the domain. Any vector in this eigenspace will be transformed radially from the origin when you draw the transformation.  If the eigenvalue is zero, then the eigenspace will be part of the null space.

An important fact is that eigenvectors corresponding to different eigenvalues will always be linearly independent.  The full proof of this fact requires knowledge of the Vandermonde matrix, and is studied in graduate mathematics. We can discuss why this works if we consider only 2 eigenvectors.  Suppose $\vec x_1,\vec x_2$ correspond to distinct eigenvalues $\lambda_1,\lambda_2$.  Suppose in addition that $\vec x_1,\vec x_2$ are linearly dependent, which means one is a linear combination of the other $\vec x_1=c\vec x_2$.  Multiplying both sides by $A$ gives $\lambda_1 \vec x_1 = c\lambda_2\vec x_2 = \lambda_2 (c\vec x_2) = \lambda_2\vec x_1.$ This means that $\lambda_1=\lambda_2$, contrary to our assumption. Hence the two vectors must be linearly independent.

\subsection{Casting Shadows using the Null Space}

The null space of a matrix tells us the directions which get mapped to zero.  When the sun shines down on you during the afternoon, it casts a shadow on the ground.  Suppose you are positioned at the origin standing with your head up along the $z$ axis.  The sun lies with its center on a unique line through the origin, so we can pick a basis vector $(x,y,z)$ to describe the position of the sun.  As the sun casts shadows upon the ground (the $xy$ plane), all of the points along the line to the sun get mapped to the origin.  Hence, this line from the origin to the sun is precisely the null space of some matrix (there are in fact infinitely many matrices with this null space). We need to find a matrix that maps $(1,0,0)$ to $(1,0)$, maps $(0,1,0)$ to $(0,1)$ and maps $(x,y,z)$ to $(0,0)$. Computer graphic artists use this fact to cast shadows. Once we know a basis for the null space, we can create a matrix that has the null space we desire, and then transform every other point in 3D onto the plane in 2D by using this matrix.

\begin{example}
Suppose that the sun is positioned on a line containing the vector $(2,1,4)$.  
We seek a transformation that has this vector as a basis for the kernel.  
Because we're mapping from 3D onto the $xy$ plane, we need a $2$ by $3$ matrix. 
Divide every component by $4$ (to get a 1 in the $z$ component). The vector $(1/2,1/4,1)$ looks a lot like the vectors we obtain by having a free variable $z$.  The null space is all linear combinations of this vector, so we write $$\bm{x\\y\\z} = z\bm{1/2\\1/4\\1}. $$ We now have two equations $x=1/2 z$ and $y=1/4z$, or $x-\frac12 z=0$ and $y-\frac14 z=0$.  The matrix $A$ we need is precisely $$A=\bm{1&0&-1/2\\0&1&-1/4},$$ obtained by writing the coefficient matrix for the two equations. 
\marginpar{
\includegraphics[width=\marginparwidth]{04-Linear-Transformations/support/LTshadow}

Knowing the null space is sufficient to cast shadows onto the $xy$ plane.
}
Notice that $(1,0,0)$ is mapped to $(1,0)$ and $(0,1,0)$ is mapped to $(0,1)$, because when an object is already in the $xy$ plane the shadow should not move. 
The image on the right illustrates this projection. 
\end{example}


\section{Composition of Linear Transformations}
The composition of two functions $f$ and $g$ is a new function $(g\circ f)(x)$ defined by $(g\circ f)(x) = g(f(x))$.  The idea is that you input an $x$ value into $f$, and then input $f(x)$ into $g$.  The same principle applies to linear transformations.  If $S:U\to V$ and $T:V\to W$ are two linear transformations, then the composition $T\circ S:U\to W$ is defined by $T(S(\vec u))$ (where $S(\vec u)$ is a vector in $V$ which serves as an input to $T$). Let's look at an application of function composition. 

\begin{example}
If we want to rotate an object 90 degrees counterclockwise, and then mirror image the object about the $y$-axis, we can achieve this by composing two linear transformations. \marginpar{When multiple linear transformations are involved, we often use the notation $S_A$ or $T_A$ to represent the standard matrix representations.}
The linear transformation $S$ which rotates and object in the plane 90 degrees has standard matrix representation $S_A = \bm{0&-1\\1&0}$, since $S(1,0) = (0,1)$ and $S(0,1)=(-1,0)$.  
The linear transformation $T$ which mirror images objects about the $y$ axis has standard matrix representation $T_A = \bm{1&0\\0&-1}$, since $T(1,0)=(1,0)$ and $T(0,1)=(0,-1)$.  The composition $T(S(x,y))$ will first perform the rotation $S(x,y)$ and then reflect the result about the $y$ axis. We compute $S(T(x,y))$ by computing 
$$S_A\left(T_A\bm{x\\y}\right) = \bm{1&0\\0&-1}\left(\bm{0&-1\\1&0}\bm{x\\y}\right) = \bm{1&0\\0&-1}\bm{0&-1\\1&0}\bm{x\\y}. $$
Notice that function composition is precisely matrix multiplication.
\end{example}


The composition of two linear transformations has as its standard matrix representation the product of the standard matrix representations of each transformation. 
In other words, if $S: U\to V$ and $T:V\to W$ are both linear transformations with standard matrices $S_A$ and $T_A$, then $T\circ S:U \to V$ (sometimes just written $TS$) has standard matrix $(TS)_A=T_AS_A$. 
Notice that in order for this composition to make sense, then the range of $S$ and domain of $T$ must match, which means that the number of rows of $A$ must equal the number of columns of $B$ (in other words you have to be able to compute the matrix product $BA$). 
Matrix multiplication was purposefully created so that the product of matrices equals the composition of linear transformations. 

When the domain and range of a linear transformation have the same dimension, the standard matrix is a square matrix.  
In such cases, we can compute the determinant of a transformation, find the eigenvalues and eigenvectors of a transformation, and ask if the transformation is invertible. 
If the transformation $T$ is invertible with standard matrix $A$, then the standard matrix of the inverse is $A^{-1}$. Symbolically we write $T^{-1}(\vec x) = A^{-1}\vec x$. 
All of the ideas we have learned up to this point about matrices can immediately be applied to linear transformations. 


\section{Elementary Matrices and Determinants}

In the patterns chapter, theorem \ref{thm det product} stated that $|AB|=|A||B|$. The determinant of a linear transformation measures how much the transformation ``stretches'' space. If one transformation $S$ stretches space by 2 units, and another transformation $T$ stretches space by $3$ units, then applying the first transformation followed by the second should stretch space by $2\cdot 3$ units. In other words, the product of the determinants of linear transformations is precisely the determinant of the composition of the linear transformations. This is a geometric reason as to why $|AB|=|A||B|$. 
A formal proof of this fact requires breaking a matrix up into a product of elementary matrices. 
The remainder of this unit focuses on elementary matrices, which are directly related to row reduction.  
Elementary matrices will help use understand exactly how to create linear transformations by composing lots of simple transformations. 
In addition, we will discover a computationally efficient way of finding determinants of large matrices.  
Let's get started.

\begin{definition}
An elementary matrix is a matrix obtained by performing one of the following row operations to the identity matrix.
\begin{enumerate}
	\item Switch any two rows.
	\item Multiply a row by a nonzero constant.
	\item Add a multiple of one row to another.
\end{enumerate}
\end{definition}
The row operations above are precisely the operations we use to perform Gaussian elimination. 
Each row operations above is invertible as well, and the inverse is found by reversing the row operation.  
\begin{enumerate}
	\item The inverse of switching two rows is to switch the same rows again.  
	\item The inverse of multiply a row by $k$ is to multiply the same row by $1/k$.  
	\item The inverse of adding $k$ times row $i$ to row $j$ is to subtract $k$ times row $i$ from row $j$. 
\end{enumerate}
Three examples of elementary matrices follow, along with their inverses.
\begin{center}
\begin{tabular}{ccc}
Switch $R_1$ and $R_2$
&
$\begin{bmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{bmatrix}^{-1} = 
\begin{bmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{bmatrix}$
\\
Multiply $R_1$ by 2
&
$\begin{bmatrix}
2&0&0\\
0&1&0\\
0&0&1
\end{bmatrix}^{-1}=
\begin{bmatrix}
\frac{1}{2}&0&0\\
0&1&0\\
0&0&1
\end{bmatrix}
$
\\
Add $4R_3$ to $R_1$
&
$\begin{bmatrix}
1&0&4\\
0&1&0\\
0&0&1
\end{bmatrix}^{-1}=
\begin{bmatrix}
1&0&-4\\
0&1&0\\
0&0&1
\end{bmatrix}
$
\end{tabular}
\end{center}
Determinants of elementary matrices are simple as well. 
\begin{enumerate}
	\item Switching two rows results in a determinant of $-1$. You are just swapping two variables, which means that you are changing the orientation, not stretching space. 
	\item Multiplying a row by a nonzero constant results in a determinant equal to the constant. It's a triangular matrix, so just multiply the diagonal entries. Visually this can be seen as stretching space by a constant. 
	\item Adding a multiple of one row to another row results in a determinant of 1. Again we have a triangular matrix, so multiply the diagonal entries. Visually this results in a shear transformation, one in which you are pushing a part of an object in a direction without changing the volume. 
\end{enumerate}

With elementary matrices, finding inverses and determinants is easy.  We now state a key theorem which allows us to connect elementary matrices to Gaussian elimination.  

\begin{theorem}
Multiplying the matrix $A$ on the left by an elementary matrix $E$ is the same as performing on $A$ the row operation used to obtain $E$.
\end{theorem}
The reason this works is because multiplying a matrix $A$ on the left by $B$ is equivalent to computing linear combinations of the rows of $A$ using the scalars in the rows of $B$. 
(Before proceeding, try making your own example to illustrate this theorem. 
This is the key way to learn to read technical documents.)  

\begin{example}
Consider the matrix 
$A=\begin{bmatrix}
1&2&3\\
0&-1&3\\
2&-4&0
\end{bmatrix}$. 
If we multiply the matrix $A$ on the left by 
$\begin{bmatrix}
1&0&4\\
0&1&0\\
0&0&1
\end{bmatrix}$, 
we obtain 
$\begin{bmatrix}
9&-14&3\\
0&-1&3\\
2&-4&0\end{bmatrix}$. 
This is the same as adding 4 times row 3 to row 1 of $A$.  
\end{example}

\subsection{Pivotal Condensation}

We will now use elementary matrices to obtain an efficient algorithm for finding the determinant of large matrices.  This process uses row reduction, and requires keeping track of each step in the forward phase of Gaussian elimination. Numerical algorithms such as the $LU$ decomposition, or $LUP$ decomposition, use essentially the ideas presented below to compute determinants, inverses, and solve systems of equations.  We'll focus on how elementary matrices simplify the process of computing determinants.

We know that multiplying a matrix $A$ on the left by an elementary matrix $E$ is equivalent to performing a row operation on $A$. 
If $A$ reduces to the identity matrix $I$, we can repeatedly multiply by elementary matrices on the left (one matrix for each row operation) to obtain an equation $$E_n \cdots E_2E_1A=I.$$ 
Recall that the inverse of a product of matrices is found by inverting each matrix and reversing the order of the product, $(AB)^{-1} = B^{-1}A^{-1}$. 
We now multiply both sides of the equation by  
$$(E_n \cdots E_2E_1)\inv = E_1^{-1}E_2^{-1}\cdots E_n^{-1},$$ 
\marginpar{If $A$ reduces to the identity matrix, then $A$ is the product of elementary matrices.} 
and then solve for $A$ to obtain 
$$A=E_1^{-1}E_2^{-1}\cdots E_n^{-1}.$$  
Since the determinant of a product is the product of the determinants, we now have 
$$|A|=|E_1^{-1}||E_2^{-1}|\cdots |E_n^{-1}|.$$ 
If we stop the row reduction once we have obtained an upper triangular matrix $U$ (once we have put zeros in the lower left corner), then we could write $$A=E_1^{-1}E_2^{-1}\cdots E_n^{-1}U.$$  The determinant of $A$ is simply the product of the determinants of each elementary matrix multiplied by the determinant of the triangular matrix $U$, i.e. $$|A|=|E_1^{-1}||E_2^{-1}|\cdots |E_n^{-1}| |U|.$$ Let's illustrate how to find the determinant using row reduction with an example, and then state some obervations as a theorem.

\begin{example}
Consider the matrix $A=
\begin{bmatrix}
0&-1&3\\
1&2&3\\
2&-4&0
\end{bmatrix}
$.  
Let's find the determinant of $A$ using row reduction.
We reduce $A$ to an upper triangular matrix as follows:
$$\begin{bmatrix}
0&-1&3\\
1&2&3\\
2&-4&0
\end{bmatrix}
\stackrel{\xrightarrow{R_1 \leftrightarrow R_2}}{E_1}
\begin{bmatrix}
1&2&3\\
0&-1&3\\
2&-4&0
\end{bmatrix}
\stackrel{\xrightarrow{-2R_1 + R_3\to R_3}}{E_2}
\begin{bmatrix}
 1 & 2 & 3 \\
 0 & -1 & 3 \\
 0 & -8 & -6
\end{bmatrix}
\stackrel{\xrightarrow{-8R_2+R_3\to R_3}}{E_3}
\begin{bmatrix}
 1 & 2 & 3 \\
 0 & -1 & 3 \\
 0 & 0 & -30
\end{bmatrix}
$$
So far we can write this reduction process as the following product of elementary matrices.
$$E_3E_2E_1A=
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & -8 & 1
\end{bmatrix}
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 -2 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
 0 & 1 & 0 \\
 1 & 0 & 0 \\
 0 & 0 & 1
\end{bmatrix}
A=\begin{bmatrix}
 1 & 2 & 3 \\
 0 & -1 & 3 \\
 0 & 0 & -30
\end{bmatrix}.
$$
Solving for $A$ we obtain (remember to invert each elementary matrix and reverse the order)
$$
A=E_1^{-1}E_2^{-1}E_3^{-1}\begin{bmatrix}
 1 & 2 & 3 \\
 0 & -1 & 3 \\
 0 & 0 & -30
\end{bmatrix}=
\begin{bmatrix}
 0 & 1 & 0 \\
 1 & 0 & 0 \\
 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 +2 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & +8 & 1
\end{bmatrix}
\begin{bmatrix}
 1 & 2 & 3 \\
 0 & -1 & 3 \\
 0 & 0 & -30
\end{bmatrix}.
$$
The determinant of $A$ is the product of the determinants of the elementary matrices and the upper triangular matrix, $$|A|=|E_1^{-1}||E_2^{-1}||E_3^{-1}||U|=(-1)(1)(1)(30)=-30,$$ where the 30 is simply the product of the diagonal entries of $U$. 
Notice that if we just kept track of the fact that we only switched rows once, and that we never multiplied any row by a nonzero constant, then we could have found this determinant by just multiplying the determinant of our upper triangular matrix by $(-1)$ (one row swap).
\end{example}  

\begin{theorem}[Determinants through Row Reduction]
Let $A$ be a square matrix. Row reduce $A$ to an upper triangular matrix $U$, and let $E_1, E_2, \ldots, E_n$ be the corresponding elementary matrices obtained from each row operation.  
Then $E_n\cdots E_2E_1A=U$, or equivalently $A=E_1\inv E_2\inv \cdots E_n\inv U$, which means the determinant of $A$ is $|A|=|E_1|\inv |E_2|\inv \cdots |E_n|\inv |U|$.  
\end{theorem}

Let's illustrate how to compute determinants using row reduction without writing the elementary matrices, and do it on a larger matrix. Computers use this algorithm to find determinants of large matrices, as the cofactor expansion method requires approximately $n!$ computations, which is computationally impossible for large values of $n$. When $n=30$ (a fairly small matrix compared to what industry uses) $30!=265252859812191058636308480000000 = 2.65\times 10^32$, which even at a trillion operations per second will still take $8.4\times 10^12$ years to compute. Pivotal condensations makes the computation of the determinant of a 30 by 30 matrix doable in seconds (it just take a while to enter the 900 entries).

\begin{example}
Consider the matrix $A=
\begin{bmatrix}
 0 & 3 & 4 & -2 \\
 -1 & 2 & -2 & 1 \\
 2 & 1 & 5 & 3 \\
 1 & 4 & 2 & -1
\end{bmatrix}$.  
We now reduce $A$ to an upper triangular matrix and then find the determinant.  
\begin{align*}
\begin{bmatrix}
 0 & 3 & 4 & -2 \\
 -1 & 2 & -2 & 1 \\
 2 & 1 & 5 & 3 \\
 1 & 4 & 2 & -1
\end{bmatrix}
\stackrel{\xrightarrow{R_1\leftrightarrow R_4}}{E_1}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 -1 & 2 & -2 & 1 \\
 2 & 1 & 5 & 3 \\
 0 & 3 & 4 & -2 
\end{bmatrix}
\stackrel{\xrightarrow{R_1+R_2\to R_2}}{E_2}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 6 & 0 & 0 \\
 2 & 1 & 5 & 3 \\
 0 & 3 & 4 & -2 
\end{bmatrix}
\\
\stackrel{\xrightarrow{-2R_1+R_3\to R_3}}{E_3}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 6 & 0 & 0 \\
 0 & -7 & 1 & 5 \\
 0 & 3 & 4 & -2 
\end{bmatrix}
\stackrel{\xrightarrow{R_2\frac{1}{6}\to R_2}}{E_4}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 1 & 0 & 0 \\
 0 & -7 & 1 & 5 \\
 0 & 3 & 4 & -2 
\end{bmatrix}
\stackrel{\xrightarrow{7R_2+R_3\to R_3}}{E_5}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 5 \\
 0 & 3 & 4 & -2 
\end{bmatrix}
\\
\stackrel{\xrightarrow{-3R_2+R_4\to R_4}}{E_6}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 5 \\
 0 & 0 & 4 & -2 
\end{bmatrix}
\stackrel{\xrightarrow{-4R_3+R_4\to R_4}}{E_7}
\begin{bmatrix}
 1 & 4 & 2 & -1\\
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 5 \\
 0 & 0 & 0 & -22 
\end{bmatrix}
\end{align*}
The determinants of the elementary matrices are all 1 except for $|E_1|=-1$ and $|E_4|=\frac{1}{6}$.  
The determinant of the upper triangular matrix at the last stage is -22 (the product of the diagonals).    
Since this upper triangular matrix is  $U=E_7E_6E_5E_4E_3E_2E_1A$, we know that $(1)(1)(1)(\frac{1}{6})(1)(1)(-1)|A|=-22$. Solving for $|A|$ gives us $|A|=(-6)(-22)=132$. 

\end{example}

To find a determinant using row reduction, all you have to do is reduce the matrix to an upper triangular matrix. Then find the determinant of this upper triangular matrix, and modify it as follows: 
\begin{enumerate}
	\item Each time you you interchanged rows, change the sign of the determinant. 
	\item Each time you  multiply a row by a nonzero constant, divide the determinant of your upper triangular matrix by this constant.  
\end{enumerate}
This will give you the determinant of your original matrix.  Notice that adding a multiple of one row to another does not change the determinant, since the corresponding elementary matrix has determinant 1.


%If there is time, add in the LU decomposition, and maybe even the LUP decomposition.  This would be great, but perhaps not what we will do.


%I notice that I completely skipped the proof that $|EA|=|E||A|$.  This would be all I need to prove the general theorem.  Should I bother with it? I could give a geometric argument (proof by picture) but not make it a formal proof.  This might satisfy many.  I could do it with 2D and 3D pictures.  Start with a simple matrix, and then illustrate the 3 row operations and their effect on area and volume.  I think I will do this (provided I have time).